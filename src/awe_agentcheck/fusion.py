from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timezone
import hashlib
import json
from pathlib import Path
import shutil
import zipfile


IGNORED_PATH_PARTS = {
    ".git",
    ".agents",
    ".venv",
    "__pycache__",
    ".pytest_cache",
    ".ruff_cache",
    ".mypy_cache",
    ".idea",
    ".vscode",
    "node_modules",
}


@dataclass(frozen=True)
class FusionResult:
    source_path: str
    target_path: str
    copied_files: list[str]
    deleted_files: list[str]
    changed_files: list[str]
    snapshot_path: str
    changelog_path: str
    merged_at: str
    mode: str


class AutoFusionManager:
    def __init__(self, *, snapshot_root: Path):
        self.snapshot_root = Path(snapshot_root)
        self.snapshot_root.mkdir(parents=True, exist_ok=True)

    def build_manifest(self, root: Path) -> dict[str, str]:
        base = Path(root)
        manifest: dict[str, str] = {}
        for path in self._iter_files(base):
            rel = path.relative_to(base).as_posix()
            manifest[rel] = self._hash_file(path)
        return manifest

    def run(
        self,
        *,
        task_id: str,
        source_root: Path,
        target_root: Path,
        before_manifest: dict[str, str],
    ) -> FusionResult:
        source = Path(source_root)
        target = Path(target_root)
        if not source.exists() or not source.is_dir():
            raise ValueError(f"source_root must be existing directory: {source}")
        if not target.exists() or not target.is_dir():
            raise ValueError(f"target_root must be existing directory: {target}")

        after_manifest = self.build_manifest(source)
        changed_files = sorted(
            [rel for rel in set(before_manifest) | set(after_manifest) if before_manifest.get(rel) != after_manifest.get(rel)]
        )
        deleted_files = sorted([rel for rel in before_manifest if rel not in after_manifest])
        copied_files: list[str] = []

        source_resolved = source.resolve()
        target_resolved = target.resolve()
        mode = "in_place" if source_resolved == target_resolved else "cross_repo"

        if mode == "cross_repo":
            for rel in changed_files:
                src = source / rel
                dst = target / rel
                if src.exists():
                    dst.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(src, dst)
                    copied_files.append(rel)
            for rel in deleted_files:
                dst = target / rel
                if dst.exists() and dst.is_file():
                    dst.unlink()

        merged_at = datetime.now(timezone.utc).isoformat()
        if not changed_files and not deleted_files:
            return FusionResult(
                source_path=str(source),
                target_path=str(target),
                copied_files=[],
                deleted_files=[],
                changed_files=[],
                snapshot_path="",
                changelog_path="",
                merged_at=merged_at,
                mode="no_changes",
            )
        snapshot_path = self._write_snapshot(
            task_id=task_id,
            merged_at=merged_at,
            target_root=target,
            changed_files=changed_files,
            deleted_files=deleted_files,
            copied_files=copied_files,
            mode=mode,
        )
        changelog_path = self._append_changelog(
            task_id=task_id,
            merged_at=merged_at,
            source_root=source,
            target_root=target,
            changed_files=changed_files,
            deleted_files=deleted_files,
            copied_files=copied_files,
            snapshot_path=snapshot_path,
            mode=mode,
        )

        return FusionResult(
            source_path=str(source),
            target_path=str(target),
            copied_files=copied_files,
            deleted_files=deleted_files,
            changed_files=changed_files,
            snapshot_path=str(snapshot_path),
            changelog_path=str(changelog_path),
            merged_at=merged_at,
            mode=mode,
        )

    def _append_changelog(
        self,
        *,
        task_id: str,
        merged_at: str,
        source_root: Path,
        target_root: Path,
        changed_files: list[str],
        deleted_files: list[str],
        copied_files: list[str],
        snapshot_path: Path,
        mode: str,
    ) -> Path:
        changelog = target_root / "CHANGELOG.auto.md"
        if not changelog.exists():
            changelog.write_text(
                "# Auto Merge Changelog\n\n"
                "Generated by awe-agentcheck fusion pipeline.\n\n",
                encoding="utf-8",
            )

        lines: list[str] = []
        lines.append(f"## {merged_at} | {task_id}")
        lines.append("")
        lines.append(f"- mode: `{mode}`")
        lines.append(f"- source: `{source_root}`")
        lines.append(f"- target: `{target_root}`")
        lines.append(f"- changed_files: `{len(changed_files)}`")
        lines.append(f"- copied_files: `{len(copied_files)}`")
        lines.append(f"- deleted_files: `{len(deleted_files)}`")
        lines.append(f"- snapshot: `{snapshot_path}`")
        if changed_files:
            preview = ", ".join(changed_files[:10])
            if len(changed_files) > 10:
                preview += ", ..."
            lines.append(f"- changed_preview: `{preview}`")
        lines.append("")

        with changelog.open("a", encoding="utf-8") as f:
            f.write("\n".join(lines))
            f.write("\n")
        return changelog

    def _write_snapshot(
        self,
        *,
        task_id: str,
        merged_at: str,
        target_root: Path,
        changed_files: list[str],
        deleted_files: list[str],
        copied_files: list[str],
        mode: str,
    ) -> Path:
        stamp = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
        archive = self.snapshot_root / f"{task_id}-{stamp}.zip"
        meta = {
            "task_id": task_id,
            "merged_at": merged_at,
            "target_root": str(target_root),
            "changed_files": changed_files,
            "copied_files": copied_files,
            "deleted_files": deleted_files,
            "mode": mode,
        }
        with zipfile.ZipFile(archive, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
            zf.writestr("meta.json", json.dumps(meta, ensure_ascii=True, indent=2))
            for rel in changed_files:
                file_path = target_root / rel
                if file_path.exists() and file_path.is_file():
                    zf.write(file_path, arcname=f"files/{rel}")
        return archive

    def _iter_files(self, root: Path):
        for path in root.rglob("*"):
            if not path.is_file():
                continue
            rel_parts = path.relative_to(root).parts
            if any(part in IGNORED_PATH_PARTS for part in rel_parts):
                continue
            yield path

    @staticmethod
    def _hash_file(path: Path) -> str:
        hasher = hashlib.sha256()
        with path.open("rb") as f:
            while True:
                chunk = f.read(1024 * 1024)
                if not chunk:
                    break
                hasher.update(chunk)
        return hasher.hexdigest()
